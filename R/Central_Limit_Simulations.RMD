---
title: "Central Limit Simulations"
author: "by : Chris at Savvy Analytics"
output:
  html_document:
    df_print: paged
    code_folding: hide        
---

<span style="color:blue"><font size="3">Background</font></span></br>

In a recent presentation with some customers, they asked why we made a lot of assumptions about data that had to do with it being normally distributed.  They made the correct observation that not all data is normally distributed. 

The Central Limit Theorem helps us overcome this problem. It says that even if the original data is not normal, the mean of many samples from that data will be normal.

This allows us to use techniques and models that work well with normal data, such as Z-Scores, T-Scores and P-Values, to describe and analyze the sample mean.

This may sound complicated, but it is easier to understand with some examples. Letâ€™s look at some data that is not normal and see how the Central Limit Theorem works by taking random samples from it.
 
```{r include=FALSE}
## 1 - Libraries, Environment, Custom Functions and Parameter Defaults
# Optional memory clear
rm(list=ls())
# Disable Scientific Notation in printing
options(scipen=999)
# Unload All Packages
lapply(names(sessionInfo()$otherPkgs), function(pkgs)
  detach(
    paste0('package:', pkgs),
    character.only = T,
    unload = T,
    force = T
  ))

QuietLoad <- function(library) {
  suppressWarnings(suppressPackageStartupMessages(
    library(library, character.only=TRUE)))
}

# Load libraries
QuietLoad('tidyverse')
QuietLoad('kableExtra')
QuietLoad('scales')
QuietLoad('gganimate')
QuietLoad('gridExtra')


RJETBlue = "#003365"

PrettyTable = function(TableObject, TableTitle) {
    TableObject %>%
      kable("html", escape = FALSE,
            caption = paste0('<p style="color:black; font-size:18px">',
            TableTitle,
            '</p>')) %>%
        kable_styling("striped",
                      bootstrap_options = c("hover", "condensed"),
                      full_width = T) %>%
        row_spec(0, color = "white", background = RJETBlue) 
}

PrettyModelTable = function(ModelObject, TableTitle) {
    summary(ModelObject)$coefficients %>%
      round(3) %>%
      data.frame() %>%
      rownames_to_column() %>%
      rename(Variable = 1, StdError = 3, TValue = 4, PValue = 5) %>%
      bind_rows(tibble(Variable = " R Squared",
                       Estimate = summary(ModelObject)$r.squared,
                       StdError = NA, TValue = NA, PValue = NA)) %>%
      arrange(Variable) %>%
      mutate(Significant = case_when(is.na(PValue) ~ "",
                                 PValue <= .05 & !Variable == "(Intercept)" ~ "Yes",
                                 TRUE ~"")) %>%
    kable("html", escape = FALSE,
          caption = paste0('<p style="color:black; font-size:18px">',
          TableTitle,
          '</p>')) %>%
      kable_styling("striped",
                    bootstrap_options = c("hover", "condensed"),
                    full_width = T) %>%
      row_spec(0, color = "white", background = RJETBlue) 
}

ModelQuartiles = function(BasicLM) {
  BasicLM = OriginalLM
  #rm(BasicLM, QuartileDF)
  QuartileDF = tibble(
  Actual = BasicLM$model[,1],
  Pred = BasicLM$fitted.values,
  ActRank = PercRank(BasicLM$model[,1])) %>%
  mutate(ActQuartile = 
          case_when(
            ActRank <= 0.25 ~ "1st Quartile",
            ActRank <= 0.50 ~ "2nd Quartile",
            ActRank <= 0.75 ~ "3rd Quartile",
            ActRank  > 0.75 ~ "4th Quartile",
            TRUE ~ "Error"),
         PredQuartile = 
          case_when(
            Pred <= quantile(BasicLM$model[,1], 0.25) ~ "1st Quartile",
            Pred <= quantile(BasicLM$model[,1], 0.50) ~ "2nd Quartile",
            Pred <= quantile(BasicLM$model[,1], 0.75) ~ "3rd Quartile",
            Pred  > quantile(BasicLM$model[,1], 0.75) ~ "4th Quartile",
            TRUE ~ "Error"),
         Success = if_else(ActQuartile == PredQuartile, 1, 0)) %>%
  group_by(ActQuartile) %>%
  summarize(Count = n(),
            Success = sum(Success),
            Accuracy = Success / Count,
            .groups = "drop")  
}

ModelDF = function(ModelObject) {
  PValue = summary(ModelObject)$coefficients %>%
  round(3) %>%
  data.frame() %>%
  rownames_to_column() %>%
  filter(rowname == "WOM_OverallScore") %>%
  pull(5)
  RSquared = summary(ModelObject)$r.squared 
  
  tibble(
    PValue = PValue,
    RSquared = RSquared
  )
  
}

PercRank <- function(x) trunc(rank(x))/length(x)


```

**Left Skewed Distribution**

```{r}

ActualDF = tibble(X = seq(1, 10, by = 0.01)) %>%
  mutate(Y = X^2 - 1 * X + 100)

ActualPlot = ActualDF %>%
  ggplot() +
  geom_histogram(aes(Y), bins = 30, fill = "dark blue") +
  theme_classic() +
  theme(plot.title = element_text(size=11)) +
  labs(title = "Left Skewed Distribution - Actual Values",
       x = "", y = "Count")

ActualDF_Sample = tibble(
  S = sapply(1:10000, function(i) mean(sample(ActualDF$Y, size = 50, replace = TRUE)))
)

SamplePlot = ActualDF_Sample %>%
  ggplot() +
  geom_histogram(aes(S), bins = 30, fill = "pink") +
  theme_classic() +
  theme(plot.title = element_text(size=11)) +
  labs(title = "10K Sample Means from Skewed Dist.",
       x = "", y = "Count")

grid.arrange(ActualPlot, SamplePlot, ncol=2,
#             top="Central Limit Theorem - Simulation")
             top="CLT - Simulation")

```

So after taking the means of 10,000 random samples of 50 from our skewed distribution we end up with a normally distributed sample distribution in pink on the right.

**Right Skewed Distribution**

```{r}

ActualDF = tibble(X = seq(1, 10, by = 0.01)) %>%
  mutate(Y = X^2 - 1 * X + 20,
         Y = max(Y)*1.5 - Y)

ActualPlot = ActualDF %>%
  ggplot() +
  geom_histogram(aes(Y), bins = 30, fill = "dark blue") +
  theme_classic() +
  theme(plot.title = element_text(size=11)) +
  labs(title = "Right Skewed Distribution - Actual Values",
       x = "", y = "Count")

ActualDF_Sample = tibble(
  S = sapply(1:10000, function(i) mean(sample(ActualDF$Y, size = 50, replace = TRUE)))
)

SamplePlot = ActualDF_Sample %>%
  ggplot() +
  geom_histogram(aes(S), bins = 30, fill = "pink") +
  theme_classic() +
  theme(plot.title = element_text(size=11)) +
  labs(title = "10K Sample Means from Skewed Dist.",
       x = "", y = "Count")

grid.arrange(ActualPlot, SamplePlot, ncol=2,
#             top="Central Limit Theorem - Simulation")
             top="CLT - Simulation")

```

So after taking the means of 10,000 random samples of 50 from our skewed distribution we end up with a normally distributed sample distribution in pink on the right.

**Bi-modal Distribution**

```{r}

ActualDF = bind_rows(
  tibble(Y = rnorm(500, mean = 100, sd = 15)),
  tibble(Y = rnorm(500, mean = 150, sd = 15)),
)

ActualPlot = ActualDF %>%
  ggplot() +
  geom_histogram(aes(Y), bins = 30, fill = "dark blue") +
  theme_classic() +
  theme(plot.title = element_text(size=11)) +
  labs(title = "Bimodal Distribution - Actual Values",
       x = "", y = "Count")

ActualDF_Sample = tibble(
  S = sapply(1:10000, function(i) mean(sample(ActualDF$Y, size = 50, replace = TRUE)))
)

SamplePlot = ActualDF_Sample %>%
  ggplot() +
  geom_histogram(aes(S), bins = 30, fill = "pink") +
  theme_classic() +
  theme(plot.title = element_text(size=11)) +
  labs(title = "10K Sample Means from Bimodal Dist.",
       x = "", y = "Count")


grid.arrange(ActualPlot, SamplePlot, ncol=2,
#             top="Central Limit Theorem - Simulation")
             top="CLT - Simulation")

```

Finally, after taking the means of 10,000 random samples of 50 from our bi-modal distribution we end up with a normally distributed sample distribution in pink on the right.

So from all of these examples you can see that we can apply the assumptions and statistical methods of normal distributions to nearly any actual distribution.

Keep in mind that these methods are useful in those cases where you have:

1. A limited amount of the total data available and need to draw conclusions about the data that you don't have.

2. Have all of the data to-date but need to draw conclusions about the data that you will collect in the future.

In those rare cases where you are just making observations about data where you have 100% of the data then these methods aren't useful or even required as you don't need to make assumptions.

Regardless of the shape of your data's distribution, you should...

Be Savvy


